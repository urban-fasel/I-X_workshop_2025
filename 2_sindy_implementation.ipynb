{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "815c86d2-358c-4119-a61e-cf1206f64070",
      "cell_type": "code",
      "source": "## 2. Implementing SINDy from Scratch\n\n# Import libraries\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.interpolate import interp1d\n\n# Load the data generated in the previous step\ndata = np.load('lotka_volterra_data.npz')\nt = data['t']\nX = data['X']\n\n# Step 1: Compute numerical derivatives\n# Using a simple finite difference for demonstration\ndt = t[1] - t[0]\nX_dot = np.gradient(X, axis=0) / dt\n\n# Step 2: Build the library of candidate functions\ndef build_library(X, poly_degree=2):\n    \"\"\"\n    Builds a polynomial library of candidate functions.\n    e.g., for X = [x, y], a degree-2 library would be [1, x, y, x^2, xy, y^2]\n    \"\"\"\n    n_samples, n_vars = X.shape\n    poly_features = []\n    \n    # Simple polynomial expansion\n    for i in range(n_samples):\n        features_i = [1]\n        for v in range(n_vars):\n            # Add linear terms\n            features_i.append(X[i, v])\n            # Add squared terms\n            for p in range(v, n_vars):\n                features_i.append(X[i, v] * X[i, p])\n        poly_features.append(features_i)\n\n    return np.array(poly_features)\n\nTheta = build_library(X)\n\n# Step 3: Use sparse regression (e.g., L1 regularization)\n# This is a simplified implementation of a sparse regression algorithm\ndef sparsify_dynamics(Theta, X_dot, lam=0.02):\n    \"\"\"\n    Finds a sparse solution to Theta * Xi = X_dot using STRidge.\n    \"\"\"\n    n_vars = X_dot.shape[1]\n    Xi = np.linalg.lstsq(Theta, X_dot, rcond=None)[0] # Initial guess\n\n    for _ in range(10): # Iterative thresholding\n        small_coeffs = np.abs(Xi) < lam\n        Xi[small_coeffs] = 0\n        for i in range(n_vars):\n            big_coeffs_idx = np.abs(Xi[:, i]) >= lam\n            if np.sum(big_coeffs_idx) > 0:\n                Xi[big_coeffs_idx, i] = np.linalg.lstsq(\n                    Theta[:, big_coeffs_idx],\n                    X_dot[:, i],\n                    rcond=None\n                )[0]\n    return Xi\n\n# Run the sparse regression\nXi = sparsify_dynamics(Theta, X_dot, lam=0.1)\n\n# Step 4: Interpret the results\n# The `Xi` matrix contains the coefficients for each term.\nprint(\"Discovered Coefficients (Xi matrix):\\n\", np.round(Xi, 2))\n\n# Let's map coefficients to equations\nfeature_names = ['1', 'x', 'y', 'x^2', 'xy', 'y^2']\nprint(\"\\nDiscovered Equations:\")\nfor i, var_name in enumerate(['dx/dt', 'dy/dt']):\n    equation = f\"{var_name} = \"\n    terms = []\n    for j, coeff in enumerate(Xi[:, i]):\n        if np.abs(coeff) > 1e-4:\n            terms.append(f\"{coeff:.2f} * {feature_names[j]}\")\n    equation += \" + \".join(terms)\n    print(equation)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}